{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio as ipy_audio\n",
    "from IPython.core.display import display\n",
    "\n",
    "from quicktranscribe import tonic, pitch, wave, kde\n",
    "from mogra import tonnetz\n",
    "from mogra.datatypes import Swar, normalize_frequency, ratio_to_swar, SWAR_BOUNDARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tonnetz Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "gs = tonnetz.EFGenus.from_list([3,3,3,3,5,5])\n",
    "tn = tonnetz.Tonnetz(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the adjacency and equivalence matrices\n",
    "adjac = tn.adjacency_matrix()\n",
    "equiv = tn.equivalence_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock Bhoop Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Continuous Time Process (batch of 2)\n",
    "# xx = np.array([\n",
    "#     [0, 2, 2, 4, 4, 4, 4, 7, 7, 7, 4, 4, 2, 2, 0, 0, 9, 9, 9, 9, 7, 7, 0, 0, 4],\n",
    "#     [7, 7, 4, 2, 0, 0, 0, 0, 2, 2, 4, 4, 7, 7, 9, 9, 9, 9, 7, 7, 4, 4, 2, 2, 0],\n",
    "# ])\n",
    "# # Labels (batch of 2)\n",
    "# true_nodes = [(0,0), (1,0), (0,1), (-1,1), (-2,1)]\n",
    "# true_node_indices = [tn.node_coordinates.index(ii) for ii in true_nodes]\n",
    "# yy = np.zeros((len(tn.node_coordinates), 2), dtype=int)\n",
    "# for ii in true_node_indices:\n",
    "#     yy[ii, 0] = 1\n",
    "#     yy[ii, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch Tracked Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sample_and_tonic(track_path, start_min=5, end_min=10):\n",
    "    \n",
    "    ctonic = tonic.read_tonic(track_path + \".ctonic.txt\")\n",
    "    y_stereo, sr = wave.read_audio_section(track_path + \".mp3\", int(start_min*60), int(end_min*60))\n",
    "    y_sample = librosa.to_mono(y_stereo.T)\n",
    "    \n",
    "    return y_sample, sr, ctonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(track):\n",
    "    # downsample\n",
    "    track = track[::16]\n",
    "    # drop nans\n",
    "    track = track[~np.isnan(track)]\n",
    "    # round to nearest int\n",
    "    track = np.round(track).astype(int)\n",
    "    \n",
    "    return track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"concrete-demo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = \"AjoyChakrabarty\"\n",
    "raags_and_times = {\n",
    "    # \"Bhoop\": [(4,6), (7,9), (10,12), (13,15), (16,18), (19,21)],\n",
    "    # \"Deshkar\": [(4,6), (7,9), (10,12), (13,15), (16,18), (19,21)],\n",
    "    \"Bhoop\": [(3,21)],\n",
    "    \"Deshkar\": [(3,21)],\n",
    "}\n",
    "# TODO: convert to Tonnetz coordinates\n",
    "raag_gt_ratios = {\n",
    "    \"Bhoop\": [1, 10/9, 5/4, 3/2, 5/3],\n",
    "    \"Deshkar\": [1, 9/8, 81/64, 3/2,  27/16],\n",
    "    \"Yaman\": [1, 9/8, 5/4, 45/32, 3/2, 27/16, 15/8],\n",
    "}\n",
    "raag_gt_nodes = {\n",
    "    \"Bhoop\": [(0,0), (1,0), (0,1), (-1,1), (-2,1)],\n",
    "    \"Deshkar\": [(0,0), (1,0), (2,0), (3,0), (4,0)],\n",
    "    \"Yaman\": [(0,0), (1,0), (2,0), (1,1), (2,1), (0,1), (3,0)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = OrderedDict({rr:[] for rr in raags_and_times.keys()})\n",
    "for raag in raags_and_times.keys():\n",
    "    track_mp3 = glob.glob(DATA_DIR + f\"*{raag}*{artist}.mp3\")[0]\n",
    "    track_path = track_mp3[:-4]\n",
    "    for start_min, end_min in tqdm(raags_and_times[raag]):\n",
    "        y_sample, sr, ctonic = read_sample_and_tonic(track_path, start_min, end_min)\n",
    "        ftrack = pitch.track_pitch_pyin(y_sample, sr, ctonic)\n",
    "        seqs[raag].append(preprocess(ftrack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mogra.tgnn import TemporalGNN, swarwise_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(tn.node_coordinates)\n",
    "input_dim = 1  # Feature dimension\n",
    "hidden_dim = 16  # Hidden layer size\n",
    "num_classes = 12  # Number of equivalence classes\n",
    "num_time_steps = 25  # Number of time steps\n",
    "batch_size = 4  # Number of samples in each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjac = torch.tensor(adjac, dtype=torch.long)\n",
    "equiv = torch.tensor(equiv, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array from seqs\n",
    "min_length = min([len(ii) for ii in seqs[\"Bhoop\"] + seqs[\"Deshkar\"]])\n",
    "xx = np.array([ii[:min_length] for raag, values in seqs.items() for ii in values])\n",
    "\n",
    "# xx is currently num_samples x sample_length and contains node indices, make it num_samples x num_nodes x sample_length x input_dim with one-hot node encoding\n",
    "xseq = np.zeros((xx.shape[0], num_nodes, xx.shape[1], input_dim))\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        xseq[i, xx[i, j], j, 0] = 1\n",
    "xseq = torch.tensor(xseq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_indices = lambda nodes: [tn.node_coordinates.index(ii) for ii in nodes]\n",
    "def tn_indices(nodes):\n",
    "    tni = np.zeros(len(tn.node_coordinates), dtype=int)\n",
    "    for ii in [tn.node_coordinates.index(ii) for ii in nodes]:\n",
    "        tni[ii] = 1\n",
    "    return tni\n",
    "yy = np.array([tn_indices(raag_gt_nodes[raag]) for raag, values in seqs.items() for ii in values])\n",
    "\n",
    "# we want to stratify yy labels into 12 classes, with each equivalence class having one (or no) labels\n",
    "yt = torch.tensor(yy, dtype=torch.long)\n",
    "yraag = torch.tensor(np.zeros((yt.shape[0], 12)), dtype=torch.long)\n",
    "for ii, mask in enumerate(equiv.T):\n",
    "    for jj in range(yt.shape[0]):\n",
    "        yraag[jj, ii] = (yt[jj, :] * mask).argmax() if sum(yt[jj, :] * mask) > 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xseq.shape, yraag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader:\n",
    "# we have num_samples of length num_nodes x sample_length x input_dim\n",
    "# we want to break down into num_time_steps from sample_length, and return in batches of batch_size\n",
    "# so we want to return \"x\"s of batch_size x num_nodes x num_time_steps x input_dim and \"y\"s of batch_size x num_classes\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, xseq, yraag, num_time_steps):\n",
    "        self.xseq = xseq\n",
    "        self.yraag = yraag\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.seqs_per_sample = xseq.shape[2] // num_time_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.xseq.shape[0] * self.seqs_per_sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = idx // self.seqs_per_sample\n",
    "        insample_idx = idx % self.seqs_per_sample\n",
    "        x = self.xseq[sample_idx, :, insample_idx*self.num_time_steps:(insample_idx+1)*self.num_time_steps, :]\n",
    "        y = self.yraag[sample_idx]\n",
    "        return x, y\n",
    "\n",
    "dataset = SeqDataset(xseq, yraag, num_time_steps)\n",
    "# split into train and val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = TemporalGNN(input_dim, hidden_dim, num_classes, num_time_steps)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "# optimizer.zero_grad()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0\n",
    "    for x, y in train_dataloader:\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        out = model(x, adjac, equiv)\n",
    "        loss = swarwise_loss(out, y, batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    # print(\"train loss: \", train_loss)\n",
    "    train_losses.append(train_loss/len(train_dataloader))\n",
    "        \n",
    "    val_loss = 0\n",
    "    for x, y in val_dataloader:\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        out = model(x, adjac, equiv)\n",
    "        loss = swarwise_loss(out, y, batch_size)\n",
    "        val_loss += loss.item()\n",
    "    # print(\"eval loss: \", val_loss)\n",
    "    val_losses.append(val_loss/len(val_dataloader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(out_scores, threshold):\n",
    "    bs = out_scores.shape[0]\n",
    "    out_cs = out_scores.view(bs*12, -1)\n",
    "    out_cs_argmax = out_cs.argmax(dim=1)\n",
    "    out_cs_max = out_cs.max(dim=1)\n",
    "    for ii, score in enumerate(out_cs_max.values):\n",
    "        if score < threshold:\n",
    "            out_cs_argmax[ii] = -1\n",
    "    out_preds = out_cs_argmax.view(bs, 12)\n",
    "    return out_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "for x, y in val_dataloader:\n",
    "    out = model(x, adjac, equiv)\n",
    "    preds.extend(get_preds(out, 0.5).flatten().detach().numpy())\n",
    "    labels.extend(y.flatten().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(labels, preds, labels=np.arange(-1, 45, 2))\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', vmax=15, xticklabels=False, yticklabels=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why doesn't more data work? why doesn't a different batch size work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges_b = adjac.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "# node_features_t = x[:, :, 0, :]\n",
    "# node_embeddings_t = F.relu(model.gcn1(node_features_t, edges_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gcn1(node_features_t[:2,:,:], edges_b[:2,:,:])\n",
    "# model.gcn1(node_features_t[:3,:,:], edges_b[:3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "# gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "# model.gcn1(node_features_t[:3,:,:], edges_b[:3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icm-shruti-analysis-XL3d-GDY-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
