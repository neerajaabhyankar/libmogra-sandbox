{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddb2328",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efe71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import subprocess\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio as ipy_audio\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc0e2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "sys.path.append(SCRIPT_DIR)\n",
    "\n",
    "from quicktranscribe import tonic, pitch, wave, kde\n",
    "from mogra import tonnetz\n",
    "from mogra.datatypes import Swar, normalize_frequency, ratio_to_swar, SWAR_BOUNDARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ad980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntonic comma in the 0 to 1 scale\n",
    "SYNTONIC_COMMA = (librosa.hz_to_midi(220*81/80) - librosa.hz_to_midi(220))/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0a7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5798e7f5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94412270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_audio(ra, savedir):\n",
    "    for raag, vv in ra.items():\n",
    "        for artist, url in vv.items():\n",
    "            command = f\"/opt/homebrew/bin/yt-dlp {url} -f 'ba' -x --audio-format 'mp3' --ffmpeg-location /opt/homebrew/bin/ffmpeg -P {savedir}/ -o {raag}-{artist}.mp3\"\n",
    "            result = subprocess.run(command, shell=True, capture_output=True)\n",
    "            print(result.stdout.decode())\n",
    "            if len(result.stderr) > 0:\n",
    "                print(\"Error:\", result.stderr.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28293b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_tonic(track_path, plot=False):\n",
    "    DEFAULT_TONIC = 220\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    start=7*60\n",
    "    end=8*60\n",
    "    y_stereo, sr = wave.read_audio_section(track_path + \".mp3\", start, end)\n",
    "    y_sample = librosa.to_mono(y_stereo.T)\n",
    "    \n",
    "    kde_sample = kde.extract(y_sample, sr=sr, tonic=DEFAULT_TONIC)\n",
    "    peaks, _ = kde.prominence_based_peak_finder(kde_sample, prominence=0.005)\n",
    "    print(peaks)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(np.linspace(0, 12, len(kde_sample)), kde_sample, color=\"teal\")\n",
    "        plt.plot(np.array(peaks) * 12/len(kde_sample), kde_sample[peaks], \"o\", markersize=\"3\", color=\"orange\")\n",
    "    \n",
    "    display(ipy_audio(y_sample, rate=sr))\n",
    "    input(\"hear the audio and press any key to continue\")\n",
    "    \n",
    "    peaks = sorted(peaks, key=lambda x: kde_sample[x], reverse=True)\n",
    "    found_tonic = False\n",
    "    for peak in peaks:\n",
    "        # generate a sine wave of the peak frequency and play it\n",
    "        fpeak = librosa.midi_to_hz(librosa.hz_to_midi(DEFAULT_TONIC) + 12 * peak / len(kde_sample))\n",
    "        ypeak = librosa.tone(fpeak, duration=3)\n",
    "        display(ipy_audio(ypeak, rate=sr))\n",
    "        ft = input(\"Is this the tonic? (y/n): \")\n",
    "        if ft == \"y\":\n",
    "            found_tonic = True\n",
    "            break\n",
    "    \n",
    "    if not found_tonic:\n",
    "        print(\"No tonic found\")\n",
    "        return None\n",
    "    \n",
    "    # write tonic to file\n",
    "    tonic.write_tonic(track_path + \".ctonic.txt\", fpeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa714665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sample_and_tonic(track_path):\n",
    "    \n",
    "    ctonic = tonic.read_tonic(track_path + \".ctonic.txt\")\n",
    "    # metadata = tonic.read_metadata(track_path + \".json\")\n",
    "    # pitch_annotations, aps = pitch.read_pitch(track_path + \".pitch.txt\")\n",
    "    \n",
    "    # # full audio\n",
    "    # y_sample, sr = wave.get_audio(track_path + \".mp3\")\n",
    "\n",
    "    # # 3-minute sample\n",
    "    start=5*60\n",
    "    end=8*60\n",
    "    y_stereo, sr = wave.read_audio_section(track_path + \".mp3\", start, end)\n",
    "    y_sample = librosa.to_mono(y_stereo.T)\n",
    "    # ipy_audio(data=y_sample, rate=sr)\n",
    "    \n",
    "    return y_sample, sr, ctonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ee1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(track):\n",
    "    # downsample\n",
    "    track = track[::16]\n",
    "    # drop nans\n",
    "    track = track[~np.isnan(track)]\n",
    "    # round to nearest int\n",
    "    track = np.round(track).astype(int)\n",
    "    \n",
    "    return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba853a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c0f6db9",
   "metadata": {},
   "source": [
    "## Tonnetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c845b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "gs = tonnetz.EFGenus.from_list([3,3,3,3,5,5])\n",
    "tn = tonnetz.Tonnetz(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the adjacency and equivalence matrices\n",
    "adjac = tn.adjacency_matrix()\n",
    "equiv = tn.equivalence_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18019dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d47d99",
   "metadata": {},
   "source": [
    "## Raags & Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a8bd9",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"concrete-demo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49609ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = \"AjoyChakrabarty\"\n",
    "raags_and_times = {\n",
    "    \"Bhoop\": [(3,21)],\n",
    "    \"Deshkar\": [(3,21)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = OrderedDict({rr:[] for rr in raags_and_times.keys()})\n",
    "for raag in raags_and_times.keys():\n",
    "    track_mp3 = glob.glob(DATA_DIR + f\"*{raag}*{artist}.mp3\")[0]\n",
    "    track_path = track_mp3[:-4]\n",
    "    for start_min, end_min in tqdm(raags_and_times[raag]):\n",
    "        y_sample, sr, ctonic = read_sample_and_tonic(track_path, start_min, end_min)\n",
    "        ftrack = pitch.track_pitch_pyin(y_sample, sr, ctonic)\n",
    "        seqs[raag].append(preprocess(ftrack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"seqs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(seqs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"seqs.pkl\", \"rb\") as f:\n",
    "#     seqs = OrderedDict(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e61f2",
   "metadata": {},
   "source": [
    "Define + Plot Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raag_gt_ratios = {\n",
    "    \"Bhoop\": [1, 10/9, 5/4, 3/2, 5/3],\n",
    "    \"Deshkar\": [1, 9/8, 81/64, 3/2,  27/16],\n",
    "    \"Yaman\": [1, 9/8, 5/4, 45/32, 3/2, 27/16, 15/8],\n",
    "}\n",
    "raag_gt_nodes = {\n",
    "    \"Bhoop\": [(0,0), (1,0), (0,1), (-1,1), (-2,1)],\n",
    "    \"Deshkar\": [(0,0), (1,0), (2,0), (3,0), (4,0)],\n",
    "    \"Yaman\": [(0,0), (1,0), (2,0), (1,1), (2,1), (0,1), (3,0)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "for raag, gt_nodes in raag_gt_nodes.items():\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=tn.coords3d[0],\n",
    "        y=tn.coords3d[1],\n",
    "        mode=\"text+markers\",\n",
    "        marker=dict(\n",
    "            size=21,\n",
    "            symbol=\"circle\",\n",
    "            color=[\"#e0b724\" if coord in gt_nodes else \"midnightblue\" for coord in tn.node_coordinates]\n",
    "        ),\n",
    "        text=tn.node_names,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(family=\"Overpass\", size=13, color=\"white\"),\n",
    "    )])\n",
    "    \n",
    "    # fig = tn.prep_plot(fig)\n",
    "    fig.update_layout(title=f\"Raag {raag}\", xaxis_title=\"powers of 3\", yaxis_title=\"powers of 5\")\n",
    "    # set major ticks\n",
    "    fig.update_xaxes(tickvals=np.arange(-4, 5))\n",
    "    fig.update_yaxes(tickvals=np.arange(-2, 3))\n",
    "    # fig.update_xaxes(tickvals=np.arange(-2, 3), ticktext=[f\"$3^{ii}$\" for ii in np.arange(-2, 3)])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4267966",
   "metadata": {},
   "source": [
    "Annotate Tonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887f948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY ONCE\n",
    "# fetch_audio(raags_and_artists, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439dc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY ONCE\n",
    "# for raag, vv in raags_and_artists.items():\n",
    "#     raag_samples = {}\n",
    "#     for artist, _ in vv.items():\n",
    "#         track_mp3 = glob.glob(f\"{DATA_DIR}/{raag}-{artist}*.mp3\")[0]\n",
    "#         annotate_tonic(track_mp3[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee3b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a3a4e6c",
   "metadata": {},
   "source": [
    "---\n",
    "The following is a Work In Progress\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fd703",
   "metadata": {},
   "source": [
    "## TGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deffde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mogra.tgnn import TemporalGNN, swarwise_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72b155",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb92005",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(tn.node_coordinates)\n",
    "input_dim = 1  # Feature dimension\n",
    "hidden_dim = 16  # Hidden layer size\n",
    "num_classes = 12  # Number of equivalence classes\n",
    "num_time_steps = 25  # Number of time steps\n",
    "batch_size = 2  # Number of samples in each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d6e85",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285932c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjac = torch.tensor(adjac, dtype=torch.long)\n",
    "equiv = torch.tensor(equiv, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debda3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array from seqs\n",
    "min_length = min([len(ii) for ii in seqs[\"Bhoop\"] + seqs[\"Deshkar\"]])\n",
    "xx = np.array([ii[:min_length] for raag, values in seqs.items() for ii in values])\n",
    "\n",
    "# xx is currently num_samples x sample_length and contains node indices, make it num_samples x num_nodes x sample_length x input_dim with one-hot node encoding\n",
    "xseq = np.zeros((xx.shape[0], num_nodes, xx.shape[1], input_dim))\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        tone_options, _ = tn.get_swar_options(Swar(xx[i, j]%12).name)\n",
    "        for tone_option in tone_options:\n",
    "            xseq[i, tn.node_coordinates.index(tone_option), j, 0] = 1\n",
    "xseq = torch.tensor(xseq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_indices = lambda nodes: [tn.node_coordinates.index(ii) for ii in nodes]\n",
    "def tn_indices(nodes):\n",
    "    tni = np.zeros(len(tn.node_coordinates), dtype=int)\n",
    "    for ii in [tn.node_coordinates.index(ii) for ii in nodes]:\n",
    "        tni[ii] = 1\n",
    "    return tni\n",
    "yy = np.array([tn_indices(raag_gt_nodes[raag]) for raag, values in seqs.items() for ii in values])\n",
    "\n",
    "# we want to stratify yy labels into 12 classes, with each equivalence class having one (or no) labels\n",
    "yt = torch.tensor(yy, dtype=torch.long)\n",
    "yraag = torch.tensor(np.zeros((yt.shape[0], 12)), dtype=torch.long)\n",
    "for ii, mask in enumerate(equiv.T):\n",
    "    for jj in range(yt.shape[0]):\n",
    "        yraag[jj, ii] = (yt[jj, :] * mask).argmax() if sum(yt[jj, :] * mask) > 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ad11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xseq.shape, yraag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610040f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader:\n",
    "# we have num_samples of length num_nodes x sample_length x input_dim\n",
    "# we want to break down into num_time_steps from sample_length, and return in batches of batch_size\n",
    "# so we want to return \"x\"s of batch_size x num_nodes x num_time_steps x input_dim and \"y\"s of batch_size x num_classes\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, xseq, yraag, num_time_steps):\n",
    "        self.xseq = xseq\n",
    "        self.yraag = yraag\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.seqs_per_sample = xseq.shape[2] // num_time_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.xseq.shape[0] * self.seqs_per_sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = idx // self.seqs_per_sample\n",
    "        insample_idx = idx % self.seqs_per_sample\n",
    "        x = self.xseq[sample_idx, :, insample_idx*self.num_time_steps:(insample_idx+1)*self.num_time_steps, :]\n",
    "        y = self.yraag[sample_idx]\n",
    "        return x, y\n",
    "\n",
    "dataset = SeqDataset(xseq, yraag, num_time_steps)\n",
    "# split into train and val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0646dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26335309",
   "metadata": {},
   "source": [
    "Instantiate model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9dfa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = TemporalGNN(input_dim, hidden_dim, num_classes, num_time_steps)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02769261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "# optimizer.zero_grad()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0\n",
    "    for x, y in train_dataloader:\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        try:\n",
    "            out = model(x, adjac, equiv)\n",
    "        except:\n",
    "            continue\n",
    "        loss = swarwise_loss(out, y, batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    # print(\"train loss: \", train_loss)\n",
    "    train_losses.append(train_loss/len(train_dataloader))\n",
    "        \n",
    "    val_loss = 0\n",
    "    for x, y in val_dataloader:\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        try:\n",
    "            out = model(x, adjac, equiv)\n",
    "        except:\n",
    "            continue\n",
    "        loss = swarwise_loss(out, y, batch_size)\n",
    "        val_loss += loss.item()\n",
    "    # print(\"eval loss: \", val_loss)\n",
    "    val_losses.append(val_loss/len(val_dataloader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4576ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad6855",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(out_scores, threshold):\n",
    "    bs = out_scores.shape[0]\n",
    "    out_cs = out_scores.view(bs*12, -1)\n",
    "    out_cs_argmax = out_cs.argmax(dim=1)\n",
    "    out_cs_max = out_cs.max(dim=1)\n",
    "    for ii, score in enumerate(out_cs_max.values):\n",
    "        if score < threshold:\n",
    "            out_cs_argmax[ii] = -1\n",
    "    out_preds = out_cs_argmax.view(bs, 12)\n",
    "    return out_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "for x, y in val_dataloader:\n",
    "    out = model(x, adjac, equiv)\n",
    "    preds.extend(get_preds(out, 0.5).flatten().detach().numpy())\n",
    "    labels.extend(y.flatten().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(labels, preds, labels=np.arange(-1, 45, 2))\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', vmax=15, xticklabels=False, yticklabels=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f9fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb23a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebf6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icm-shruti-analysis-XL3d-GDY-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
