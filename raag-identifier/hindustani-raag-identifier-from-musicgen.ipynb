{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from typing import Dict, Any, List, Union, Optional\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import librosa\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import MusicgenForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.path.abspath(\"\")))\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"\"))+\"/ToolsISMIR2024/syntheory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ToolsISMIR2024.syntheory.embeddings.models import (\n",
    "    Model, load_musicgen_model, load_audio,\n",
    "    mfcc, melspectrogram, chroma_cqt, concat_features,\n",
    ")\n",
    "from ToolsISMIR2024.syntheory.embeddings.extract_embeddings import audio_file_to_embedding_np_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"neerajaabhyankar/hindustani-raag-small\"\n",
    "hrs_full = load_dataset(dataset_id, revision=\"0dfb021e54e0e7489b90a47e23ef15f34fa740ec\")\n",
    "hrs = hrs_full[\"train\"].train_test_split(seed=42, shuffle=True, train_size=0.8, test_size=0.2, stratify_by_column=\"label\") # train-val split\n",
    "del hrs_full\n",
    "dataset_name = dataset_id.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 48000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config = {\n",
    "#     \"model_name\": \"MELSPEC\",\n",
    "#     \"model_type\": \"MELSPEC\",\n",
    "#     \"minimum_duration_in_sec\": 4,\n",
    "# }\n",
    "# processor, model = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"model_name\": \"MUSICGEN_DECODER_LM_L\",\n",
    "    \"model_type\": \"MUSICGEN_DECODER_LM_L\",\n",
    "    \"minimum_duration_in_sec\": 4,\n",
    "    # \"extract_from_layer\": None,\n",
    "}\n",
    "processor, model = load_musicgen_model(Model.MUSICGEN_DECODER_LM_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload\n",
    "# import importlib\n",
    "# importlib.reload(sys.modules[\"ToolsISMIR2024.syntheory.embeddings.extract_embeddings\"])\n",
    "# importlib.reload(sys.modules[\"transformers\"])\n",
    "# from transformers import MusicgenForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_audio(y: np.ndarray, orig_sr: int, target_sr: int, duration: Optional[float] = None) -> np.ndarray:\n",
    "    # truncate\n",
    "    if duration is not None:\n",
    "        y = y[:int(duration * orig_sr)]\n",
    "    \n",
    "    # resample\n",
    "    audio = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr)\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio[np.newaxis]\n",
    "    audio = audio.mean(axis=0)\n",
    "\n",
    "    # normalize audio\n",
    "    norm_factor = np.abs(audio).max()\n",
    "    if norm_factor > 0:\n",
    "        audio /= norm_factor\n",
    "\n",
    "    return audio.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_musicgen_emb(\n",
    "    audio: np.ndarray,\n",
    "    processor: AutoProcessor,\n",
    "    model: Union[MusicgenForConditionalGeneration],\n",
    "    model_config: Dict[str, Any],\n",
    "):\n",
    "    # parse config\n",
    "    extract_from_layer=model_config.get(\"extract_from_layer\", None)\n",
    "    decoder_hidden_states=model_config.get(\"decoder_hidden_states\", True)\n",
    "    meanpool=model_config.get(\"meanpool\", True)\n",
    "    model_type=Model[model_config[\"model_type\"]]\n",
    "    \n",
    "    # set up inputs\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate  # MusicGen uses 32000 Hz\n",
    "    audio = resample_audio(audio, SR, sampling_rate)\n",
    "    print(\"audio resampled\")\n",
    "    \n",
    "    # process inputs for model\n",
    "    inputs = processor(\n",
    "        audio=audio,\n",
    "        text=\"\",\n",
    "        sampling_rate=sampling_rate,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    if model.device.type == \"mps\":\n",
    "        inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
    "        inputs[\"input_values\"] = inputs[\"input_values\"].to(torch.float32)\n",
    "    print(\"inputs prepared\")\n",
    "    \n",
    "    if model_type == Model.MUSICGEN_AUDIO_ENCODER:\n",
    "        x = inputs[\"input_values\"]\n",
    "        # audio encoder\n",
    "        audio_encoder = model.get_audio_encoder()\n",
    "        # extract representations from audio encoder\n",
    "        for layer in audio_encoder.encoder.layers:\n",
    "            x = layer(x)\n",
    "        if meanpool:\n",
    "            return x.mean(axis=2).squeeze().detach().numpy()\n",
    "        else:\n",
    "            return x.squeeze().detach().numpy()\n",
    "    elif model_type in [\n",
    "        Model.MUSICGEN_DECODER_LM_S,\n",
    "        Model.MUSICGEN_DECODER_LM_M,\n",
    "        Model.MUSICGEN_DECODER_LM_L\n",
    "    ]:\n",
    "        # extract representations from decoder LM\n",
    "        out = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "        # output decoder hidden states\n",
    "        if decoder_hidden_states:\n",
    "            if extract_from_layer is None:\n",
    "                if meanpool:\n",
    "                    return np.stack(tuple(l.mean(axis=1).squeeze().detach().cpu().numpy() for l in out.decoder_hidden_states))\n",
    "                else:\n",
    "                    return np.stack(tuple(l.squeeze().detach().cpu().numpy() for l in out.decoder_hidden_states))\n",
    "            else:\n",
    "                if meanpool:\n",
    "                    return out.decoder_hidden_states[extract_from_layer].mean(axis=1).squeeze().detach().cpu().numpy()\n",
    "                else:\n",
    "                    return out.decoder_hidden_states[extract_from_layer].squeeze().detach().cpu().numpy()\n",
    "        # output decoder attentions\n",
    "        else:\n",
    "            if extract_from_layer is None:\n",
    "                if meanpool:\n",
    "                    return np.stack(tuple(l.mean(axis=(2, 3)).squeeze().detach().cpu().numpy() for l in out.decoder_attentions))\n",
    "                else:\n",
    "                    return np.stack(tuple(l.squeeze().detach().cpu().numpy() for l in out.decoder_attentions))\n",
    "            else:\n",
    "                if meanpool:\n",
    "                    return out.decoder_attentions[extract_from_layer].mean(axis=(2, 3)).squeeze().detach().cpu().numpy()\n",
    "                else:\n",
    "                    return out.decoder_attentions[extract_from_layer].squeeze().detach().cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir = \"musicgen-embeddings-hindustani-raag-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii, row in tqdm(enumerate(hrs[\"train\"])):\n",
    "#     path_key = row[\"audio\"][\"path\"].split(\"/\")[-1]\n",
    "#     if os.path.exists(f\"{embedding_dir}/{path_key}.pkl\"):\n",
    "#         continue\n",
    "#     inputs = row[\"audio\"][\"array\"]\n",
    "#     if len(inputs) / SR < model_config[\"minimum_duration_in_sec\"]:\n",
    "#         continue\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up inputs\n",
    "# sampling_rate = model.config.audio_encoder.sampling_rate  # MusicGen uses 32000 Hz\n",
    "# audio = resample_audio(inputs, SR, sampling_rate)\n",
    "# inputs2 = processor(\n",
    "#     audio=audio,\n",
    "#     text=\"\",\n",
    "#     sampling_rate=sampling_rate,\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# inputs2 = {k: v.to(\"mps\") for k, v in inputs2.items()}\n",
    "# inputs2[\"input_values\"] = inputs2[\"input_values\"].to(torch.float32)\n",
    "# out = model(**inputs2, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, row in tqdm(enumerate(hrs[\"train\"])):\n",
    "    path_key = row[\"audio\"][\"path\"].split(\"/\")[-1]\n",
    "    if os.path.exists(f\"{embedding_dir}/{path_key}.pkl\"):\n",
    "        continue\n",
    "    inputs = row[\"audio\"][\"array\"]\n",
    "    if len(inputs) / SR < model_config[\"minimum_duration_in_sec\"]:\n",
    "        continue\n",
    "    emb = extract_musicgen_emb(inputs, processor, model, model_config)[-1]\n",
    "    pickle.dump(emb, open(f\"{embedding_dir}/{path_key}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, row in tqdm(enumerate(hrs[\"test\"])):\n",
    "    path_key = row[\"audio\"][\"path\"].split(\"/\")[-1]\n",
    "    if os.path.exists(f\"{embedding_dir}/{path_key}.pkl\"):\n",
    "        continue\n",
    "    inputs = row[\"audio\"][\"array\"]\n",
    "    emb = extract_musicgen_emb(inputs, processor, model, model_config)[-1]\n",
    "    pickle.dump(emb, open(f\"{embedding_dir}/{path_key}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding(examples):\n",
    "#     input_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "#     embeddings = [get_embedding_from_model_using_config(inputs, model_config, processor, model)[-1] for inputs in input_arrays]\n",
    "#     examples[\"embeddings\"] = embeddings\n",
    "#     return examples\n",
    "\n",
    "# hrs = hrs.map(get_embedding, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(hrs[\"train\"], batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(hrs[\"test\"], batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from probe.probes import SimpleMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        hidden_layer_sizes: List[int],\n",
    "        num_outputs: int,\n",
    "        dropout_p: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        d = num_features\n",
    "\n",
    "        self.num_layers = len(hidden_layer_sizes)\n",
    "        for i, ld in enumerate(hidden_layer_sizes):\n",
    "            setattr(self, f\"hidden_{i}\", torch.nn.Linear(d, ld))\n",
    "            d = ld\n",
    "\n",
    "        self.output = torch.nn.Linear(d, num_outputs)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = getattr(self, f\"hidden_{i}\")(x)\n",
    "            x = torch.functional.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the MLP\n",
    "X = np.vstack([bhoop_train, yaman_train])\n",
    "y = np.array([0] * len(bhoop_train) + [1] * len(yaman_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 2\n",
    "\n",
    "mlp = SimpleMLP(num_features=X.shape[1], hidden_layer_sizes=[64, 32], num_outputs=2)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X, dtype=torch.float32)\n",
    "y_train = torch.tensor(y, dtype=torch.long)\n",
    "X_test = torch.tensor([bhoop_test, yaman_test], dtype=torch.float32)\n",
    "y_test = torch.tensor([0, 1], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = mlp(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        outputs = mlp(X_test)\n",
    "        loss_eval = criterion(outputs, y_test)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Eval Loss: {loss_eval.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mlp(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probes from the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe.probes import ProbeExperiment, ProbeExperimentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBE_CONFIG = {\n",
    "    \"model_hash\": None,\n",
    "    \"dataset\": None,\n",
    "    \"dataset_embeddings_label_column_name\": None,\n",
    "    \"data_standardization\": True,\n",
    "    \"hidden_layer_sizes\": [],\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dropout_p\": 0.5,\n",
    "    \"l2_weight_decay\": None,\n",
    "    \"max_num_epochs\": None,\n",
    "    \"early_stopping_metric\": \"primary\",\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_eval_frequency\": 8,\n",
    "    \"early_stopping_boredom\": 256,\n",
    "    \"seed\": 0,\n",
    "    \"num_outputs\": None,\n",
    "    # if this is true, all the embedding files used in test/train are loaded into RAM\n",
    "    # otherwise, we load only their location in a zarr file on disk and load as needed\n",
    "    \"load_embeddings_in_memory\": False,\n",
    "}\n",
    "\n",
    "def _set_attr_if_exists(probe_config, hparams, attr_name, default=None):\n",
    "    x = getattr(probe_config, attr_name, default)\n",
    "    if x is not None:\n",
    "        hparams[attr_name] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCEPT_LABELS = {\n",
    "    \"chord_progressions\": [\n",
    "        (19, \"chord_progression\"),\n",
    "        (12, \"key_note_name\"),\n",
    "    ],\n",
    "    \"chords\": [(4, \"chord_type\"), (3, \"inversion\"), (12, \"root_note_name\")],\n",
    "    \"scales\": [(7, \"mode\"), (12, \"root_note_name\")],\n",
    "    \"intervals\": [(12, \"interval\"), (12, \"root_note_name\")],\n",
    "    \"notes\": [(12, \"root_note_pitch_class\"), (9, \"octave\")],\n",
    "    \"time_signatures\": [\n",
    "        (8, \"time_signature\"),\n",
    "        (6, \"time_signature_beats\"),\n",
    "        (3, \"time_signature_subdivision\"),\n",
    "    ],\n",
    "    \"tempos\": [(161, \"bpm\")],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(\n",
    "    use_wandb: bool = False, random_seed: int = 0, base_path_parent=\"data\"\n",
    ") -> ProbeExperiment:    \n",
    "    # model type: [ JUKEBOX | MUSICGEN_DECODER | MUSICGEN_AUDIO_ENCODER | MFCC | CHROMA | MELSPEC | HANDCRAFT ]\n",
    "    model_type = \"MUSICGEN_DECODER\"\n",
    "    # model size: [S | M | L]\n",
    "    model_size = \"L\"\n",
    "    # model layer: [0, ... 71]\n",
    "    model_layer = None\n",
    "    # concept: [notes, tempos, time_signatures, etc. ] + a specific label\n",
    "    concept = \"scales\"\n",
    "\n",
    "    num_classes = getattr(PROBE_CONFIG, \"num_classes\", None)\n",
    "    # set hyperparameters\n",
    "    hparams = {}\n",
    "    _set_attr_if_exists(PROBE_CONFIG, hparams, \"data_standardization\")\n",
    "    _set_attr_if_exists(PROBE_CONFIG, hparams, \"batch_size\")\n",
    "    _set_attr_if_exists(PROBE_CONFIG, hparams, \"learning_rate\")\n",
    "    _set_attr_if_exists(PROBE_CONFIG, hparams, \"dropout_p\")\n",
    "    _set_attr_if_exists(PROBE_CONFIG, hparams, \"l2_weight_decay\")\n",
    "    _set_attr_if_exists(PROBE_CONFIG, hparams, \"hidden_layer_sizes\", [512])\n",
    "\n",
    "    # get the concept label that is given by parent concept and the target we wish to probe\n",
    "    dataset_settings = CONCEPT_LABELS[concept][0]\n",
    "    _num_classes, label_column_name = dataset_settings\n",
    "\n",
    "    # allow override of number of classes if given in config directly\n",
    "    num_classes = num_classes or _num_classes\n",
    "\n",
    "    is_regression = concept == \"tempos\"\n",
    "    num_outputs = 1 if is_regression else num_classes\n",
    "    output_type = \"regression\" if is_regression else \"multiclass\"\n",
    "\n",
    "    cfg = ProbeExperimentConfig(\n",
    "        dataset_embeddings_label_column_name=label_column_name,\n",
    "        dataset=concept,\n",
    "        num_outputs=num_outputs,\n",
    "        model_hash=f\"{model_type}-{model_size}-{model_layer}\",\n",
    "        max_num_epochs=100,\n",
    "        **hparams,\n",
    "        seed=random_seed,\n",
    "        load_embeddings_in_memory=False,\n",
    "    )\n",
    "\n",
    "    exp = ProbeExperiment(\n",
    "        cfg,\n",
    "        summarize_frequency=100,\n",
    "        use_wandb=use_wandb,\n",
    "    )\n",
    "    exp.obtain_data(\n",
    "        model_type=model_type,\n",
    "        model_size=model_size,\n",
    "        output_type=output_type,\n",
    "        model_layer=model_layer,\n",
    "    )\n",
    "    exp.train()\n",
    "\n",
    "    return exp\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icm-shruti-analysis-XL3d-GDY-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
